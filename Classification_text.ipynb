{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filnels/Homeworks/blob/main/Classification_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Загружаем датасет\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "\n",
        "csv_file_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
        "data = pd.read_csv(csv_file_path)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sooXizAvepN-",
        "outputId": "4e71a561-2fd2-400e-bb8d-56448a9ac91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training.example import Example\n",
        "from pathlib import Path\n",
        "import random\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Функция загрузки данных\n",
        "def load_training_data(\n",
        "    csv_file_path: str = \"IMDB Dataset.csv\",\n",
        "    split: float = 0.8,\n",
        "    limit: int = 0\n",
        ") -> tuple:\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "    if limit > 0:\n",
        "        data = data.sample(n=limit, random_state=42)\n",
        "    reviews = [\n",
        "        (row[\"review\"], {\"cats\": {\"pos\": row[\"sentiment\"] == \"positive\", \"neg\": row[\"sentiment\"] == \"negative\"}})\n",
        "        for _, row in data.iterrows()\n",
        "    ]\n",
        "    random.shuffle(reviews)\n",
        "    split_index = int(len(reviews) * split)\n",
        "    train_data = reviews[:split_index]\n",
        "    test_data = reviews[split_index:]\n",
        "    return train_data, test_data\n",
        "\n",
        "# Обучение модели\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 10,\n",
        "    model_output_dir: str = \"./model_output\"\n",
        ") -> None:\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.add_pipe(\"textcat\", last=True)\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "    optimizer = nlp.begin_training()\n",
        "    train_examples = [\n",
        "        Example.from_dict(nlp.make_doc(text), labels) for text, labels in training_data\n",
        "    ]\n",
        "\n",
        "    print(\"Начало обучения\")\n",
        "    losses_list = []\n",
        "    for i in range(iterations):\n",
        "        random.shuffle(train_examples)\n",
        "        losses = {}\n",
        "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
        "        for batch in batches:\n",
        "            nlp.update(batch, drop=0.2, losses=losses)\n",
        "\n",
        "        losses_list.append(losses[\"textcat\"])\n",
        "        print(f\"Итерация {i+1} - Потери: {losses['textcat']}\")\n",
        "\n",
        "        if (i + 1) % 5 == 0:\n",
        "            evaluate_model(nlp, test_data)\n",
        "\n",
        "    output_dir = Path(model_output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(f\"Модель сохранена в: {output_dir}\")\n",
        "\n",
        "# Оценка модели с использованием нескольких метрик\n",
        "def evaluate_model(nlp, test_data):\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    for text, labels in test_data:\n",
        "        true_label = \"pos\" if labels[\"cats\"][\"pos\"] == 1.0 else \"neg\"\n",
        "        true_labels.append(true_label)\n",
        "        doc = nlp(text)\n",
        "        pred_label = max(doc.cats, key=doc.cats.get)\n",
        "        pred_labels.append(pred_label)\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(\n",
        "        true_labels, pred_labels, labels=[\"pos\", \"neg\"], average=\"binary\", pos_label=\"pos\"\n",
        "    )\n",
        "    accuracy = sum(1 for true, pred in zip(true_labels, pred_labels) if true == pred) / len(true_labels)\n",
        "    print(f\"Точность: {accuracy:.4f}, Полнота: {recall:.4f}, Точность: {precision:.4f}, F-мера: {fscore:.4f}\")\n",
        "\n",
        "# Пример использования\n",
        "csv_file_path = \"/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\"\n",
        "train_data, test_data = load_training_data(csv_file_path, split=0.8, limit=50000)  # limit=5000 для быстрой проверки\n",
        "train_model(train_data, test_data, iterations=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "GMgx8CJDBF0A",
        "outputId": "42f7ac9f-9904-4457-d598-3246c7cfacc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5d8702f09c4e>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# limit=5000 для быстрой проверки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-5d8702f09c4e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(training_data, test_data, iterations, model_output_dir)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     train_examples = [\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     ]\n",
            "\u001b[0;32m<ipython-input-16-5d8702f09c4e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     train_examples = [\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     ]\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             )\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFtjH4Pxbyod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Видим, что в целом модель дет хорошие результаты, постепенно уменьшются потери, где-то на 7 итерации можно остановить обучение."
      ],
      "metadata": {
        "id": "c6E5FUwhahvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(input_data_list: list):\n",
        "    \"\"\"\n",
        "    Тестирует модель на списке текстов и выводит предсказания и вероятность для каждого текста.\n",
        "    \"\"\"\n",
        "    # Загружаем сохраненную модель\n",
        "    loaded_model = spacy.load(\"model_output\")\n",
        "\n",
        "    results = []\n",
        "    for input_data in input_data_list:\n",
        "        parsed_text = loaded_model(input_data)\n",
        "        # Определяем возвращаемое предсказание\n",
        "        if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
        "            prediction = \"Положительный отзыв\"\n",
        "            score = parsed_text.cats[\"pos\"]\n",
        "        else:\n",
        "            prediction = \"Негативный отзыв\"\n",
        "            score = parsed_text.cats[\"neg\"]\n",
        "        results.append((input_data, prediction, score))\n",
        "\n",
        "    for review, prediction, score in results:\n",
        "        print(f\"Текст обзора: {review}\\nПредсказание: {prediction}\\nScore: {score:.3f}\\n\")\n",
        "\n",
        "# Пример использования\n",
        "test_reviews = [\n",
        "    \"\"\"\n",
        "   Well I absolutely loved this. A sweet, earnest parable about parenthood, purpose and perseverance.\n",
        "   I find, as I get older, that I deeply appreciate stories that are kind and uncynical.\n",
        "   The world needs more of them, especially now. I'm glad this one exists in the world,\n",
        "   and my kids loved it too - though I think they'll experience an entirely different movie\n",
        "   if they revisit this when they're older. This one got me, and I was fighting tears on more than a few occasions. One of my favorites this year.\n",
        "    \"\"\",\n",
        "    \"This movie was a total waste of time.\",\n",
        "    \"\"\"A movie made for iPad kids to watch on 2x speed while they try to get\n",
        "     a Victory Royale fighting in Slurpy Swamp.\"\"\",\n",
        "    \"It was okay, not the best but not the worst either.\"\n",
        "]\n",
        "test_model(input_data_list=test_reviews)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ6djK2NGMRd",
        "outputId": "d5a5cdbb-fe3c-4a93-92cd-3e9dd9196be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст обзора: \n",
            "   Well I absolutely loved this. A sweet, earnest parable about parenthood, purpose and perseverance. \n",
            "   I find, as I get older, that I deeply appreciate stories that are kind and uncynical. \n",
            "   The world needs more of them, especially now. I'm glad this one exists in the world, \n",
            "   and my kids loved it too - though I think they'll experience an entirely different movie \n",
            "   if they revisit this when they're older. This one got me, and I was fighting tears on more than a few occasions. One of my favorites this year.\n",
            "    \n",
            "Предсказание: Положительный отзыв\n",
            "Score: 1.000\n",
            "\n",
            "Текст обзора: This movie was a total waste of time.\n",
            "Предсказание: Негативный отзыв\n",
            "Score: 1.000\n",
            "\n",
            "Текст обзора: A movie made for iPad kids to watch on 2x speed while they try to get\n",
            "     a Victory Royale fighting in Slurpy Swamp.\n",
            "Предсказание: Положительный отзыв\n",
            "Score: 1.000\n",
            "\n",
            "Текст обзора: It was okay, not the best but not the worst either.\n",
            "Предсказание: Негативный отзыв\n",
            "Score: 1.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_count = sum(1 for _, label in train_data if label[\"cats\"][\"pos\"] == 1.0)\n",
        "negative_count = len(train_data) - positive_count\n",
        "print(f\"Положительных отзывов: {positive_count}, Отрицательных отзывов: {negative_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bayAW2Y_b4AJ",
        "outputId": "a5b618a1-994f-4421-bf2d-d099793b4c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Положительных отзывов: 20015, Отрицательных отзывов: 19985\n"
          ]
        }
      ]
    }
  ]
}